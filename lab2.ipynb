{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "950d63f7",
   "metadata": {},
   "source": [
    "# PROG8245 Lab2 – Data Collection and Pre-processing\n",
    "\n",
    "**Group**: Group1\n",
    "\n",
    "**GitHub Link**: https://github.com/chence/DataCollectionPreProcessing.git\n",
    "\n",
    "**Team Members**:\n",
    "\n",
    "- Ce Chen | 9007166\n",
    "- Zhuoran Zhang | 9048508\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03614144",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required libraries (run once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95604f57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:28.832157Z",
     "iopub.status.busy": "2026-01-30T04:30:28.831385Z",
     "iopub.status.idle": "2026-01-30T04:30:30.026876Z",
     "shell.execute_reply": "2026-01-30T04:30:30.024112Z"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip -q install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0324272",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1206736e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.033117Z",
     "iopub.status.busy": "2026-01-30T04:30:30.032685Z",
     "iopub.status.idle": "2026-01-30T04:30:30.420868Z",
     "shell.execute_reply": "2026-01-30T04:30:30.419088Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc0c09",
   "metadata": {},
   "source": [
    "## Step 1 – Hello, Data!\n",
    "**Load raw CSV, display first 3 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803b249f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.424440Z",
     "iopub.status.busy": "2026-01-30T04:30:30.424100Z",
     "iopub.status.idle": "2026-01-30T04:30:30.439588Z",
     "shell.execute_reply": "2026-01-30T04:30:30.438258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               date   order_id customer_id       product  product_sku   price  \\\n",
      "0  2025-04-03 15:52  ORD000001    CUST0244    Gym Gloves  SPO-GG-0028  113.30   \n",
      "1  2024-10-03 08:11  ORD000002    CUST0271  Notebook Set  OFF-NS-0031  196.64   \n",
      "2  2024-07-08 21:46  ORD000003    CUST0277  Notebook Set  OFF-NS-0031  181.54   \n",
      "\n",
      "   quantity coupon_code shipping_city payment_method sales_channel  \\\n",
      "0         3        NONE      Hamilton     Google Pay        Mobile   \n",
      "1         1        NONE        Guelph     Google Pay        Mobile   \n",
      "2         2        NONE     Kitchener          Debit        Mobile   \n",
      "\n",
      "   shipping_cost  \n",
      "0           5.78  \n",
      "1           6.36  \n",
      "2           8.32  \n"
     ]
    }
   ],
   "source": [
    "primary_path = Path(\"data/primary_transactions_1000.csv\")\n",
    "secondary_path = Path(\"data/secondary_product_catalog.csv\")\n",
    "\n",
    "df_txn_raw = pd.read_csv(primary_path)[:500]\n",
    "df_meta_raw = pd.read_csv(secondary_path)\n",
    "\n",
    "print(df_txn_raw.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea09f3",
   "metadata": {},
   "source": [
    "## Step 2 – Pick the Right Container\n",
    "**Justify dict vs namedtuple vs sets(1–2 sentences)**\n",
    "\n",
    "We use a **DataFrame** for bulk row/column operations, a **dict** for fast key-based lookups/aggregations (e.g., revenue per city), and a **set** to get unique values quickly (e.g., unique city count)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ba81a",
   "metadata": {},
   "source": [
    "## Step 3 – Implement Functions and Data Structure\n",
    "**Implement and use it to populate an data structure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df3834a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.442463Z",
     "iopub.status.busy": "2026-01-30T04:30:30.442068Z",
     "iopub.status.idle": "2026-01-30T04:30:30.454920Z",
     "shell.execute_reply": "2026-01-30T04:30:30.454073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               date   order_id customer_id       product  product_sku   price  \\\n",
      "0  2025-04-03 15:52  ORD000001    CUST0244    Gym Gloves  SPO-GG-0028  113.30   \n",
      "1  2024-10-03 08:11  ORD000002    CUST0271  Notebook Set  OFF-NS-0031  196.64   \n",
      "2  2024-07-08 21:46  ORD000003    CUST0277  Notebook Set  OFF-NS-0031  181.54   \n",
      "\n",
      "   quantity coupon_code shipping_city payment_method sales_channel  \\\n",
      "0         3        NONE      Hamilton     Google Pay        Mobile   \n",
      "1         1        NONE        Guelph     Google Pay        Mobile   \n",
      "2         2        NONE     Kitchener          Debit        Mobile   \n",
      "\n",
      "   shipping_cost  \n",
      "0           5.78  \n",
      "1           6.36  \n",
      "2           8.32  \n"
     ]
    }
   ],
   "source": [
    "class TransactionProcessor:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def total(self) -> float:\n",
    "        # total gross revenue (price * quantity)\n",
    "        return float((self.df[\"price\"] * self.df[\"quantity\"]).sum())\n",
    "\n",
    "    def clean(self) -> dict:\n",
    "        \"\"\"Apply cleaning rules and return before/after counts.\"\"\"\n",
    "        df = self.df.copy()\n",
    "\n",
    "        before = {\n",
    "            \"rows\": int(len(df)),\n",
    "            \"missing_price\": int(df[\"price\"].isna().sum()),\n",
    "            \"missing_quantity\": int(df[\"quantity\"].isna().sum()),\n",
    "            \"missing_shipping_city\": int(df[\"shipping_city\"].isna().sum()),\n",
    "            \"bad_price_nonpositive\": int((pd.to_numeric(df[\"price\"], errors=\"coerce\") <= 0).sum()),\n",
    "            \"bad_quantity_nonpositive\": int((pd.to_numeric(df[\"quantity\"], errors=\"coerce\") <= 0).sum()),\n",
    "            \"bad_date_parse\": int(pd.to_datetime(df[\"date\"], errors=\"coerce\").isna().sum()),\n",
    "            \"coupon_blank\": int(df[\"coupon_code\"].astype(str).str.strip().eq(\"\").sum()),\n",
    "        }\n",
    "\n",
    "        # Rule 1) Parse date; drop rows where date can't be parsed\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"date\"]).copy()\n",
    "\n",
    "        # Rule 2) Coerce price/quantity to numeric\n",
    "        df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "        df[\"quantity\"] = pd.to_numeric(df[\"quantity\"], errors=\"coerce\")\n",
    "\n",
    "        # Rule 3) Fix non-positive or missing price -> fill with median price\n",
    "        df.loc[df[\"price\"] <= 0, \"price\"] = np.nan\n",
    "        df[\"price\"] = df[\"price\"].fillna(df[\"price\"].median())\n",
    "\n",
    "        # Rule 4) Fix non-positive or missing quantity -> fill with 1\n",
    "        df.loc[df[\"quantity\"] <= 0, \"quantity\"] = np.nan\n",
    "        df[\"quantity\"] = df[\"quantity\"].fillna(1).astype(int)\n",
    "\n",
    "        # Rule 5) Clean shipping_city -> strip; fill missing/blank with 'Unknown'\n",
    "        df[\"shipping_city\"] = df[\"shipping_city\"].astype(str).str.strip()\n",
    "        df.loc[df[\"shipping_city\"].isin([\"nan\", \"None\", \"\"]), \"shipping_city\"] = \"Unknown\"\n",
    "\n",
    "        # Rule 6) Clean coupon_code -> strip + upper; fill missing/blank with 'NONE'\n",
    "        df[\"coupon_code\"] = df[\"coupon_code\"].astype(str).str.strip().str.upper()\n",
    "        df.loc[df[\"coupon_code\"].isin([\"nan\", \"None\", \"\"]), \"coupon_code\"] = \"NONE\"\n",
    "\n",
    "        after = {\n",
    "            \"rows\": int(len(df)),\n",
    "            \"missing_price\": int(df[\"price\"].isna().sum()),\n",
    "            \"missing_quantity\": int(df[\"quantity\"].isna().sum()),\n",
    "            \"missing_shipping_city\": int(df[\"shipping_city\"].isna().sum()),\n",
    "            \"bad_price_nonpositive\": int((df[\"price\"] <= 0).sum()),\n",
    "            \"bad_quantity_nonpositive\": int((df[\"quantity\"] <= 0).sum()),\n",
    "            \"bad_date_parse\": int(df[\"date\"].isna().sum()),\n",
    "            \"coupon_blank\": int(df[\"coupon_code\"].astype(str).str.strip().eq(\"\").sum()),\n",
    "        }\n",
    "\n",
    "        self.df = df\n",
    "        return {\"before\": before, \"after\": after}\n",
    "\n",
    "proc = TransactionProcessor(df_txn_raw)\n",
    "print(proc.df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28e061",
   "metadata": {},
   "source": [
    "## Step 4 – Bulk Loaded\n",
    "**Example: Map data structures from dataframes to dictionaries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba7bca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.457267Z",
     "iopub.status.busy": "2026-01-30T04:30:30.457042Z",
     "iopub.status.idle": "2026-01-30T04:30:30.462867Z",
     "shell.execute_reply": "2026-01-30T04:30:30.462145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ELE-BS-0001': {'product_name': 'Bluetooth Speaker', 'category': 'Electronics', 'brand': 'MapleWorks', 'base_cost': 12.38, 'msrp': 24.54, 'is_fragile': True, 'is_perishable': False}, 'ELE-WE-0002': {'product_name': 'Wireless Earbuds', 'category': 'Electronics', 'brand': 'MapleWorks', 'base_cost': 77.48, 'msrp': 110.31, 'is_fragile': True, 'is_perishable': False}}\n"
     ]
    }
   ],
   "source": [
    "# Example: build a lookup dict from product_sku -> product metadata\n",
    "meta_by_sku = df_meta_raw.set_index(\"product_sku\").to_dict(orient=\"index\")\n",
    "\n",
    "# Show 2 example entries\n",
    "sample_keys = list(meta_by_sku.keys())[:2]\n",
    "print({k: meta_by_sku[k] for k in sample_keys})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fe52e",
   "metadata": {},
   "source": [
    "## Step 5 – Quick Profiling\n",
    "**Min/mean/max price, unique city count (set).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9bdd37f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.465141Z",
     "iopub.status.busy": "2026-01-30T04:30:30.464922Z",
     "iopub.status.idle": "2026-01-30T04:30:30.469842Z",
     "shell.execute_reply": "2026-01-30T04:30:30.469090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_price': 6.6, 'mean_price': 66.25408, 'max_price': 220.98, 'unique_city_count': 20}\n"
     ]
    }
   ],
   "source": [
    "price_num = pd.to_numeric(df_txn_raw[\"price\"], errors=\"coerce\")\n",
    "cities_set = set(df_txn_raw[\"shipping_city\"].astype(str).str.strip())\n",
    "\n",
    "profiling = {\n",
    "    \"min_price\": float(np.nanmin(price_num)),\n",
    "    \"mean_price\": float(np.nanmean(price_num)),\n",
    "    \"max_price\": float(np.nanmax(price_num)),\n",
    "    \"unique_city_count\": int(len(cities_set)),\n",
    "}\n",
    "print(profiling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bd6c4",
   "metadata": {},
   "source": [
    "## Step 6 – Spot the Grime\n",
    "**Identify at least three dirty data cases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d99d59d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.472175Z",
     "iopub.status.busy": "2026-01-30T04:30:30.471898Z",
     "iopub.status.idle": "2026-01-30T04:30:30.480346Z",
     "shell.execute_reply": "2026-01-30T04:30:30.479293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unparseable_dates': 0, 'nonpositive_prices': 0, 'nonpositive_quantities': 0, 'missing_or_blank_city': 0, 'missing_or_blank_coupon': 0}\n"
     ]
    }
   ],
   "source": [
    "grime_checks = {\n",
    "    \"unparseable_dates\": int(pd.to_datetime(df_txn_raw[\"date\"], errors=\"coerce\").isna().sum()),\n",
    "    \"nonpositive_prices\": int((pd.to_numeric(df_txn_raw[\"price\"], errors=\"coerce\") <= 0).sum()),\n",
    "    \"nonpositive_quantities\": int((pd.to_numeric(df_txn_raw[\"quantity\"], errors=\"coerce\") <= 0).sum()),\n",
    "    \"missing_or_blank_city\": int(df_txn_raw[\"shipping_city\"].isna().sum() + df_txn_raw[\"shipping_city\"].astype(str).str.strip().eq(\"\").sum()),\n",
    "    \"missing_or_blank_coupon\": int(df_txn_raw[\"coupon_code\"].isna().sum() + df_txn_raw[\"coupon_code\"].astype(str).str.strip().eq(\"\").sum()),\n",
    "}\n",
    "print(grime_checks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeeb2bc",
   "metadata": {},
   "source": [
    "## Step 7 – Cleaning Rules\n",
    "**Execute fixes inside clean(); show “before/after” counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2d8027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.482878Z",
     "iopub.status.busy": "2026-01-30T04:30:30.482653Z",
     "iopub.status.idle": "2026-01-30T04:30:30.494458Z",
     "shell.execute_reply": "2026-01-30T04:30:30.493573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'before': {'rows': 500, 'missing_price': 0, 'missing_quantity': 0, 'missing_shipping_city': 0, 'bad_price_nonpositive': 0, 'bad_quantity_nonpositive': 0, 'bad_date_parse': 0, 'coupon_blank': 0}, 'after': {'rows': 500, 'missing_price': 0, 'missing_quantity': 0, 'missing_shipping_city': 0, 'bad_price_nonpositive': 0, 'bad_quantity_nonpositive': 0, 'bad_date_parse': 0, 'coupon_blank': 0}}\n"
     ]
    }
   ],
   "source": [
    "clean_summary = proc.clean()\n",
    "print(clean_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b79d11b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.496679Z",
     "iopub.status.busy": "2026-01-30T04:30:30.496449Z",
     "iopub.status.idle": "2026-01-30T04:30:30.504410Z",
     "shell.execute_reply": "2026-01-30T04:30:30.503566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date   order_id customer_id       product  product_sku  \\\n",
      "0 2025-04-03 15:52:00  ORD000001    CUST0244    Gym Gloves  SPO-GG-0028   \n",
      "1 2024-10-03 08:11:00  ORD000002    CUST0271  Notebook Set  OFF-NS-0031   \n",
      "2 2024-07-08 21:46:00  ORD000003    CUST0277  Notebook Set  OFF-NS-0031   \n",
      "\n",
      "    price  quantity coupon_code shipping_city payment_method sales_channel  \\\n",
      "0  113.30         3        NONE      Hamilton     Google Pay        Mobile   \n",
      "1  196.64         1        NONE        Guelph     Google Pay        Mobile   \n",
      "2  181.54         2        NONE     Kitchener          Debit        Mobile   \n",
      "\n",
      "   shipping_cost  \n",
      "0           5.78  \n",
      "1           6.36  \n",
      "2           8.32  \n"
     ]
    }
   ],
   "source": [
    "print(proc.df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064baf10",
   "metadata": {},
   "source": [
    "## Step 8 – Transformations\n",
    "**For example: Parse coupon_code ➞ numeric discount (others apply).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "270b5ad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.506689Z",
     "iopub.status.busy": "2026-01-30T04:30:30.506393Z",
     "iopub.status.idle": "2026-01-30T04:30:30.513291Z",
     "shell.execute_reply": "2026-01-30T04:30:30.512455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  coupon_code  discount_percent\n",
      "0        NONE                 0\n",
      "1        NONE                 0\n",
      "2        NONE                 0\n",
      "3      FLASH5                 5\n",
      "4        NONE                 0\n",
      "5        NONE                 0\n",
      "6        NONE                 0\n",
      "7        NONE                 0\n",
      "8        NONE                 0\n",
      "9       VIP20                20\n"
     ]
    }
   ],
   "source": [
    "def parse_discount_percent(code: str) -> int:\n",
    "    \"\"\"Transform coupon_code into numeric discount percent.\"\"\"\n",
    "    if code is None:\n",
    "        return 0\n",
    "    code = str(code).strip().upper()\n",
    "    if code in (\"NONE\", \"\"):\n",
    "        return 0\n",
    "    if \"FREE\" in code:  # FREESHIP etc.\n",
    "        return 0\n",
    "    m = re.search(r\"(\\d+)$\", code)  # trailing digits\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "proc.df[\"discount_percent\"] = proc.df[\"coupon_code\"].apply(parse_discount_percent).astype(int)\n",
    "print(proc.df[[\"coupon_code\", \"discount_percent\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc41ffd",
   "metadata": {},
   "source": [
    "## Step 9 – Feature Engineering\n",
    "**For example: Add days_since_purchase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "918fbef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.515717Z",
     "iopub.status.busy": "2026-01-30T04:30:30.515353Z",
     "iopub.status.idle": "2026-01-30T04:30:30.524767Z",
     "shell.execute_reply": "2026-01-30T04:30:30.523887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date   price  quantity  discount_percent  gross_revenue  \\\n",
      "0 2025-04-03 15:52:00  113.30         3                 0         339.90   \n",
      "1 2024-10-03 08:11:00  196.64         1                 0         196.64   \n",
      "2 2024-07-08 21:46:00  181.54         2                 0         363.08   \n",
      "3 2024-01-21 16:38:00   42.84         1                 5          42.84   \n",
      "4 2025-05-23 15:46:00   43.10         4                 0         172.40   \n",
      "\n",
      "   net_revenue  days_since_purchase  \n",
      "0       339.90                  285  \n",
      "1       196.64                  468  \n",
      "2       363.08                  554  \n",
      "3        40.70                  723  \n",
      "4       172.40                  235  \n"
     ]
    }
   ],
   "source": [
    "# days_since_purchase relative to the most recent purchase date in the dataset\n",
    "reference_date = proc.df[\"date\"].max()\n",
    "proc.df[\"days_since_purchase\"] = (reference_date - proc.df[\"date\"]).dt.days.astype(int)\n",
    "\n",
    "# revenue columns\n",
    "proc.df[\"gross_revenue\"] = (proc.df[\"price\"] * proc.df[\"quantity\"]).round(2)\n",
    "proc.df[\"net_revenue\"] = (proc.df[\"gross_revenue\"] * (1 - proc.df[\"discount_percent\"] / 100.0)).round(2)\n",
    "\n",
    "print(proc.df[[\"date\", \"price\", \"quantity\", \"discount_percent\", \"gross_revenue\", \"net_revenue\", \"days_since_purchase\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741cd265",
   "metadata": {},
   "source": [
    "## Step 10 – Mini-Aggregation\n",
    "**For example: Revenue per shipping_city (dict or pandas.groupby).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf851ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.527027Z",
     "iopub.status.busy": "2026-01-30T04:30:30.526794Z",
     "iopub.status.idle": "2026-01-30T04:30:30.536549Z",
     "shell.execute_reply": "2026-01-30T04:30:30.535080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shipping_city\n",
      "Barrie           6630.13\n",
      "Hamilton         6262.70\n",
      "Windsor          4565.68\n",
      "Mississauga      4434.20\n",
      "Sudbury          3808.26\n",
      "Waterloo         3385.17\n",
      "Thunder Bay      3206.55\n",
      "Niagara Falls    2922.52\n",
      "Whitby           2666.48\n",
      "Burlington       2437.20\n",
      "Name: net_revenue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "revenue_per_city = proc.df.groupby(\"shipping_city\")[\"net_revenue\"].sum().round(2).sort_values(ascending=False)\n",
    "\n",
    "# Show top 10 cities\n",
    "print(revenue_per_city.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "141ef42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.539828Z",
     "iopub.status.busy": "2026-01-30T04:30:30.539579Z",
     "iopub.status.idle": "2026-01-30T04:30:30.543895Z",
     "shell.execute_reply": "2026-01-30T04:30:30.543058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barrie', 6630.13), ('Hamilton', 6262.7), ('Windsor', 4565.68), ('Mississauga', 4434.2), ('Sudbury', 3808.26)]\n"
     ]
    }
   ],
   "source": [
    "# Also demonstrate a dict result (as required option)\n",
    "revenue_city_dict = revenue_per_city.to_dict()\n",
    "print(list(revenue_city_dict.items())[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958a5de",
   "metadata": {},
   "source": [
    "## Step 11 – Serialization Checkpoint\n",
    "**Save cleaned data to JSON.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c346c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.545988Z",
     "iopub.status.busy": "2026-01-30T04:30:30.545790Z",
     "iopub.status.idle": "2026-01-30T04:30:30.559023Z",
     "shell.execute_reply": "2026-01-30T04:30:30.557916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/cleaned_transactions.json output/cleaned_transactions.csv\n"
     ]
    }
   ],
   "source": [
    "out_json = Path(\"output/cleaned_transactions.json\")\n",
    "out_csv = Path(\"output/cleaned_transactions.csv\")\n",
    "\n",
    "out_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "proc.df.to_json(out_json, orient=\"records\", indent=2, date_format=\"iso\")\n",
    "proc.df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(out_json.as_posix(), out_csv.as_posix())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dffe53",
   "metadata": {},
   "source": [
    "## Step 12 – Soft Interview Reflection\n",
    "**Markdown: < 120 words explaining how Functions have helped**\n",
    "\n",
    "\n",
    "**Our reflection:**\n",
    "\n",
    "Functions helped us make the workflow repeatable and less error‑prone. Instead of fixing issues manually each time, we wrapped cleaning rules (date parsing, numeric conversion, missing values) into a `clean()` function so we could run it consistently and compare before/after counts. We also used small transformation functions like `parse_discount_percent()` to convert coupon codes into a numeric feature. This made the code easier to test, reuse, and explain in the notebook, and it kept my analysis steps clearer and more organized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd055c2",
   "metadata": {},
   "source": [
    "## Data-Dictionary\n",
    "Merge field definitions from the primary CSV header and the secondary metadata source.\n",
    "\n",
    "Table fields: **Field, Type, Description, Source**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d730749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.562117Z",
     "iopub.status.busy": "2026-01-30T04:30:30.561886Z",
     "iopub.status.idle": "2026-01-30T04:30:30.572011Z",
     "shell.execute_reply": "2026-01-30T04:30:30.571082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Field      Type     Source  \\\n",
      "0             base_cost     float  Secondary   \n",
      "1                 brand    string  Secondary   \n",
      "2              category    string  Secondary   \n",
      "3           coupon_code    string    Primary   \n",
      "4           customer_id    string    Primary   \n",
      "5                  date  datetime    Primary   \n",
      "6   days_since_purchase       int    Primary   \n",
      "7      discount_percent       int    Primary   \n",
      "8         gross_revenue     float    Primary   \n",
      "9            is_fragile    string  Secondary   \n",
      "10        is_perishable    string  Secondary   \n",
      "11                 msrp     float  Secondary   \n",
      "12          net_revenue     float    Primary   \n",
      "13             order_id    string    Primary   \n",
      "14       payment_method    string    Primary   \n",
      "15                price     float    Primary   \n",
      "16              product    string    Primary   \n",
      "17         product_name    string  Secondary   \n",
      "18          product_sku    string    Primary   \n",
      "19             quantity       int    Primary   \n",
      "20        sales_channel    string    Primary   \n",
      "21        shipping_city    string    Primary   \n",
      "22        shipping_cost     float    Primary   \n",
      "\n",
      "                                          Description  \n",
      "0   Metadata field 'base_cost' from the product ca...  \n",
      "1    Metadata field 'brand' from the product catalog.  \n",
      "2   Metadata field 'category' from the product cat...  \n",
      "3   Transaction field 'coupon_code' from the prima...  \n",
      "4   Transaction field 'customer_id' from the prima...  \n",
      "5   Transaction field 'date' from the primary tran...  \n",
      "6   Transaction field 'days_since_purchase' from t...  \n",
      "7   Transaction field 'discount_percent' from the ...  \n",
      "8   Transaction field 'gross_revenue' from the pri...  \n",
      "9   Metadata field 'is_fragile' from the product c...  \n",
      "10  Metadata field 'is_perishable' from the produc...  \n",
      "11    Metadata field 'msrp' from the product catalog.  \n",
      "12  Transaction field 'net_revenue' from the prima...  \n",
      "13  Transaction field 'order_id' from the primary ...  \n",
      "14  Transaction field 'payment_method' from the pr...  \n",
      "15  Transaction field 'price' from the primary tra...  \n",
      "16  Transaction field 'product' from the primary t...  \n",
      "17  Metadata field 'product_name' from the product...  \n",
      "18  Transaction field 'product_sku' from the prima...  \n",
      "19  Transaction field 'quantity' from the primary ...  \n",
      "20  Transaction field 'sales_channel' from the pri...  \n",
      "21  Transaction field 'shipping_city' from the pri...  \n",
      "22  Transaction field 'shipping_cost' from the pri...  \n"
     ]
    }
   ],
   "source": [
    "def infer_type(series: pd.Series) -> str:\n",
    "    t = str(series.dtype)\n",
    "    if \"datetime\" in t:\n",
    "        return \"datetime\"\n",
    "    if \"int\" in t:\n",
    "        return \"int\"\n",
    "    if \"float\" in t:\n",
    "        return \"float\"\n",
    "    return \"string\"\n",
    "\n",
    "dd_rows = []\n",
    "\n",
    "# Primary fields\n",
    "for col in proc.df.columns:\n",
    "    dd_rows.append({\n",
    "        \"Field\": col,\n",
    "        \"Type\": infer_type(proc.df[col]),\n",
    "        \"Source\": \"Primary\",\n",
    "        \"Description\": f\"Transaction field '{col}' from the primary transactions file.\",\n",
    "    })\n",
    "\n",
    "# Secondary fields\n",
    "for col in df_meta_raw.columns:\n",
    "    dd_rows.append({\n",
    "        \"Field\": col,\n",
    "        \"Type\": infer_type(df_meta_raw[col]),\n",
    "        \"Source\": \"Secondary\",\n",
    "        \"Description\": f\"Metadata field '{col}' from the product catalog.\",\n",
    "    })\n",
    "\n",
    "data_dictionary = pd.DataFrame(dd_rows)\n",
    "\n",
    "# Remove duplicate field names (prefer Primary CSV if same name exists)\n",
    "data_dictionary = (\n",
    "    data_dictionary\n",
    "    .sort_values(by=[\"Field\", \"Source\"])\n",
    "    .drop_duplicates(subset=[\"Field\"], keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(data_dictionary.head(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4da71",
   "metadata": {},
   "source": [
    "### How new columns were created\n",
    "- `discount_percent`: extracted from `coupon_code` (trailing digits → percent; FREE*/NONE → 0).\n",
    "- `days_since_purchase`: computed using the most recent `date` as a reference.\n",
    "- `gross_revenue`: `price * quantity`.\n",
    "- `net_revenue`: `gross_revenue * (1 - discount_percent/100)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2b66f",
   "metadata": {},
   "source": [
    "## Concise Analytical Insight\n",
    "One quick insight: which shipping city generates the highest net revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee23f872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:30:30.574651Z",
     "iopub.status.busy": "2026-01-30T04:30:30.574425Z",
     "iopub.status.idle": "2026-01-30T04:30:30.578870Z",
     "shell.execute_reply": "2026-01-30T04:30:30.577898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insight: The city with the highest net revenue is Barrie, with a total net revenue of $6630.13.\n"
     ]
    }
   ],
   "source": [
    "top_city = revenue_per_city.index[0]\n",
    "top_city_revenue = float(revenue_per_city.iloc[0])\n",
    "\n",
    "# insight\n",
    "print(\n",
    "    \"Insight: \"\n",
    "    f\"The city with the highest net revenue is {top_city}, \"\n",
    "    f\"with a total net revenue of ${top_city_revenue:.2f}.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
